# ğŸ“š Smart Librarian

A sleek **FastAPI + React (Vite)** app that recommends books by text **or** voice.  
Speak your vibe â†’ ğŸ™ï¸ **STT** â†’ ğŸ¤– **RAG + LLM** â†’ ğŸ“– curated picks â†’ (in background) ğŸ”Š **TTS** + ğŸ–¼ï¸ **cover image**.  
The UI shows â€œGeneratingâ€¦â€ placeholders and swaps in media automatically when ready.

---

## âœ¨ Features

- ğŸ” **RAG** book recommendations (`/api/recommend`)
- ğŸ™ï¸ **Voice search** (browser mic â†’ `/api/stt/transcribe`)
- ğŸ”Š **TTS narration** (background, deterministic filenames)
- ğŸ–¼ï¸ **AI cover image** (background, deterministic filenames)
- âš¡ **Fast**: single OpenAI client, Chroma init once, STT disk cache
- ğŸ’¸ **Cost controls**: disable TTS / Cover via env flags
- ğŸ§± **Clean DX**: Vite proxy, Swagger docs, simple project layout

---

## ğŸ§° Tech Stack

**Backend:** FastAPI, ChromaDB, OpenAI SDK  
**Frontend:** React (Vite)  
**Media:** Whisper (STT), TTS, Image Gen

---

## âœ… Requirements

- Python **3.11+**
- Node **18+**
- An **OpenAI API key**

---

## ğŸš€ Quick Start

### 1) Backend
```bash
python -m venv .venv
# Windows
.\.venv\Scripts\activate
# macOS/Linux
source .venv/bin/activate

pip install -r requirements.txt
```
Create .env at the repo root (copy from .env.example if present):

OPENAI_API_KEY=sk-...
CHAT_MODEL=gpt-4o-mini
# Optional cost flags (1=on, 0=off)
ENABLE_TTS=1
ENABLE_COVER=1


Run the API (pick a free port, e.g. 8020):
```bash
uvicorn app.api:app --app-dir . --port 8020
```

ğŸ“– Docs: http://localhost:8020/docs

### 2) Frontend
```bash
cd frontend
npm install
```

Make sure frontend/vite.config.js proxies to your API port:
```bash
server: {
  port: 5173,
  proxy: {
    '/api': 'http://localhost:8020',
    '/static': 'http://localhost:8020'
  }
}
```

Run:
```bash
npm run dev
```

## ğŸ–¥ï¸ App: http://localhost:5173

âš™ï¸ Configuration

Environment variables (backend):

Key	Default	Notes
OPENAI_API_KEY	â€”	Required
CHAT_MODEL	gpt-4o-mini	LLM for recommendations
ENABLE_TTS	1	1/0 toggle audio generation
ENABLE_COVER	1	1/0 toggle cover generation

ğŸ—‚ï¸ Generated DB & media live under .chroma/ (gitignored).

ğŸ“¡ API (dev)
POST /api/recommend

Request

{ "query": "dark academia with friendship themes" }


Response

{
  "answer": "...",
  "title": "â€¦",
  "audio_url": "/static/stt/<hash>.mp3",
  "image_url": "/static/img/<hash>.png",
  "candidates": [["Title A", 0.12], ["Title B", 0.18]]
}

POST /api/stt/transcribe

Multipart with file (webm/ogg/mp3/wav/m4a).
Response

{ "text": "â€¦", "url": "/static/stt/<uploaded>" }

## ğŸ§  How Media Delivery Works (1-minute mental model)

The backend returns deterministic URLs for audio & image immediately.

Generation runs in a background task and saves to those exact paths.

The frontend shows â€œGeneratingâ€¦â€ and HEAD-polls those URLs until they return 200, then swaps in the real media.

Repeated requests for the same content reuse existing files (no extra API cost).

STT results are cached by file hash on disk.

## ğŸ—ƒï¸ Project Structure
app/
  api.py          # FastAPI app (routes, static mounts, background tasks)
  main.py         # Chroma init & helpers (no FastAPI here)
  tools/          # STT, TTS, image, dataset, filters, recommend
frontend/
  src/            # App.jsx, MicSearch, styles
  vite.config.js  # dev proxy â†’ backend
.chroma/          # DB + generated media (gitignored)

## ğŸ› ï¸ Troubleshooting

404 on /api/* â†’ Vite proxy points to the wrong API port. Update vite.config.js and restart both servers.

Port already in use â†’ Stop the previous process or pick a new port (e.g. 8020) and update the proxy.

Media never appears â†’ Check files land in:

Audio â†’ .chroma/stt/<hash>.mp3 (served at /static/stt/...)

Image â†’ .chroma/img/<hash>.png (served at /static/img/...)
Filenames must match the URLs returned by /api/recommend.

